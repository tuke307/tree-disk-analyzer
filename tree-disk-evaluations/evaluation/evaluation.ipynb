{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Import the required packages directly\n",
    "import treedisksegmentation\n",
    "import treediskpith\n",
    "import treediskrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "ground_truh_images_path = \"../ground-truth/images\"\n",
    "ground_truh_path = \"../ground-truth/ground_truth.json\"\n",
    "\n",
    "# Configuration settings (normally imported from config)\n",
    "OUTPUT_DIR = \"../output/\"\n",
    "INPUT_DIR = \"../input/\"\n",
    "YOLO_PITH_MODEL_PATH = \"../models/yolo11s-det-pith.pt\"\n",
    "YOLO_SEG_MODEL_PATH = \"../models/yolo11s-seg-tree.pt\"\n",
    "DEBUG = False\n",
    "SAVE_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ground truth\n",
    "ground_truth_json = json.load(open(ground_truh_path))\n",
    "\n",
    "# Create required directories if they don't exist\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# check if the model files exist\n",
    "if not os.path.exists(YOLO_PITH_MODEL_PATH):\n",
    "    logger.error(f\"Model file {YOLO_PITH_MODEL_PATH} does not exist\")\n",
    "    exit(1)\n",
    "\n",
    "if not os.path.exists(YOLO_SEG_MODEL_PATH):\n",
    "    logger.error(f\"Model file {YOLO_SEG_MODEL_PATH} does not exist\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_data):\n",
    "    \"\"\"\n",
    "    Process a single image through the entire pipeline using direct package imports:\n",
    "    1. Segmentation\n",
    "    2. Pith detection\n",
    "    3. Rings detection\n",
    "    \n",
    "    Returns the predicted age and the visualization image\n",
    "    \"\"\"\n",
    "    image_path = os.path.join(ground_truh_images_path, image_data[\"path\"])\n",
    "    \n",
    "    # # Step 1: Segmentation\n",
    "    # logger.info(f\"Running segmentation on {image_path}\")\n",
    "    \n",
    "    # # Configure and run segmentation\n",
    "    # treedisksegmentation.configure(\n",
    "    #     input_image=image_path,\n",
    "    #     model_path=YOLO_SEG_MODEL_PATH,\n",
    "    #     output_dir=OUTPUT_DIR,\n",
    "    #     save_results=SAVE_RESULTS,\n",
    "    #     debug=DEBUG,\n",
    "    # )\n",
    "    # segmented_image, masks = treedisksegmentation.run()\n",
    "    \n",
    "    # if segmented_image is None:\n",
    "    #     logger.error(f\"Segmentation failed for {image_path}\")\n",
    "    #     return None, None\n",
    "    \n",
    "    # # Convert segmented image to PIL for saving\n",
    "    # segmented_pil = Image.fromarray(segmented_image)\n",
    "    # segmented_path = f\"{OUTPUT_DIR}/segmented_{os.path.basename(image_path)}\"\n",
    "    # segmented_pil.save(segmented_path)\n",
    "    \n",
    "    # Step 2: Pith detection\n",
    "    logger.info(f\"Running pith detection on segmented image\")\n",
    "    \n",
    "    # Configure and run pith detection\n",
    "    treediskpith.configure(\n",
    "        input_image=image_path, # segmented_path\n",
    "        model_path=YOLO_PITH_MODEL_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        save_results=SAVE_RESULTS,\n",
    "        debug=DEBUG,\n",
    "    )\n",
    "    img_in, img_processed, pith = treediskpith.run()\n",
    "    \n",
    "    if pith is None:\n",
    "        logger.error(f\"Pith detection failed for {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 3: Rings detection\n",
    "    logger.info(f\"Running rings detection with pith at {pith}\")\n",
    "    \n",
    "    # Configure and run rings detection\n",
    "    treediskrings.configure(\n",
    "        input_image=image_path, # segmented_path\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        cx=int(pith[0]),\n",
    "        cy=int(pith[1]),\n",
    "        sigma=1.0,\n",
    "        th_low=5.0,\n",
    "        th_high=15.0,\n",
    "        save_results=SAVE_RESULTS,\n",
    "        debug=DEBUG,\n",
    "    )\n",
    "    \n",
    "    result = treediskrings.run_age_detect()\n",
    "    \n",
    "    if result is None:\n",
    "        logger.error(f\"Rings detection failed for {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    average_ring_count, img_out = result\n",
    "    \n",
    "    return average_ring_count, img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_plots(results, true_ages, predicted_ages, accuracy_at_tolerance):\n",
    "    \"\"\"\n",
    "    Create visualizations for better evaluation understanding\n",
    "    \"\"\"\n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # 1. Scatter plot of true age vs predicted age\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.scatter(true_ages, predicted_ages, alpha=0.7, s=80)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_age = min(min(true_ages), min(predicted_ages))\n",
    "    max_age = max(max(true_ages), max(predicted_ages))\n",
    "    ax1.plot([min_age, max_age], [min_age, max_age], 'r--', label='Perfect Prediction')\n",
    "    \n",
    "    # Add +/- 3 years tolerance lines\n",
    "    ax1.plot([min_age, max_age], [min_age + 3, max_age + 3], 'g--', alpha=0.5, label='+3 years')\n",
    "    ax1.plot([min_age, max_age], [min_age - 3, max_age - 3], 'g--', alpha=0.5, label='-3 years')\n",
    "    \n",
    "    ax1.set_xlabel('True Age (years)')\n",
    "    ax1.set_ylabel('Predicted Age (years)')\n",
    "    ax1.set_title('True Age vs. Predicted Age')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Bar chart of errors for each image\n",
    "    errors = true_ages - predicted_ages\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    bar_positions = np.arange(len(errors))\n",
    "    bars = ax2.bar(bar_positions, errors, alpha=0.7)\n",
    "    \n",
    "    # Color bars based on error magnitude\n",
    "    for i, bar in enumerate(bars):\n",
    "        if abs(errors[i]) <= 1:\n",
    "            bar.set_color('green')\n",
    "        elif abs(errors[i]) <= 3:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    ax2.set_xlabel('Image Index')\n",
    "    ax2.set_ylabel('Error (True - Predicted)')\n",
    "    ax2.set_title('Prediction Error by Image')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add image names as x-tick labels\n",
    "    image_names = [os.path.basename(r[\"path\"]) for r in results if r.get(\"predicted_age\") is not None]\n",
    "    ax2.set_xticks(bar_positions)\n",
    "    ax2.set_xticklabels(image_names, rotation=90, ha='right', fontsize=8)\n",
    "    \n",
    "    # 3. Accuracy at different tolerance levels\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    \n",
    "    tolerance_values = [int(k.split('_')[-1]) for k in accuracy_at_tolerance.keys()]\n",
    "    accuracy_values = list(accuracy_at_tolerance.values())\n",
    "    \n",
    "    ax3.plot(tolerance_values, accuracy_values, 'o-', linewidth=2, markersize=10)\n",
    "    ax3.set_xlabel('Tolerance (years)')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.set_title('Accuracy at Different Tolerance Levels')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_xticks(tolerance_values)\n",
    "    \n",
    "    # 4. Histogram of age differences\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    ax4.hist(np.abs(errors), bins=range(0, int(max(abs(errors))) + 2), alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xlabel('Absolute Error (years)')\n",
    "    ax4.set_ylabel('Number of Images')\n",
    "    ax4.set_title('Distribution of Absolute Errors')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the combined figure with Titles (set transparent background)\n",
    "    plt.savefig('evaluation_plots.png', dpi=300, transparent=True)\n",
    "    plt.savefig('evaluation_plots.pdf', transparent=True)\n",
    "    \n",
    "    # Save each subplot individually WITHOUT Title\n",
    "    fig.canvas.draw()  # Update the renderer\n",
    "    for i, ax in enumerate([ax1, ax2, ax3, ax4], start=1):\n",
    "        original_title = ax.get_title()\n",
    "        ax.set_title('')  # Remove title\n",
    "        extent = ax.get_tightbbox(fig.canvas.get_renderer()).transformed(fig.dpi_scale_trans.inverted())\n",
    "        plt.savefig(f'evaluation_plot_{i}.png', dpi=300, bbox_inches=extent, transparent=True)\n",
    "        plt.savefig(f'evaluation_plot_{i}.pdf', bbox_inches=extent, transparent=True)\n",
    "        ax.set_title(original_title)  # Optionally restore the title\n",
    "    \n",
    "    logger.info(f\"Evaluation plots saved to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculate improved evaluation metrics based on the results\n",
    "    \"\"\"\n",
    "    # Filter out results with failed predictions\n",
    "    valid_results = [r for r in results if r.get(\"predicted_age\") is not None]\n",
    "    \n",
    "    if not valid_results:\n",
    "        logger.error(\"No valid predictions to calculate metrics\")\n",
    "        return\n",
    "    \n",
    "    # Extract true and predicted ages\n",
    "    true_ages = np.array([r[\"true_age\"] for r in valid_results])\n",
    "    predicted_ages = np.array([r[\"predicted_age\"] for r in valid_results])\n",
    "    \n",
    "    # Calculate standard regression metrics\n",
    "    mae = mean_absolute_error(true_ages, predicted_ages)\n",
    "    rmse = np.sqrt(mean_squared_error(true_ages, predicted_ages))\n",
    "    r2 = r2_score(true_ages, predicted_ages)\n",
    "    \n",
    "    # Calculate mean percentage error\n",
    "    mpe = np.mean(np.abs(true_ages - predicted_ages) / true_ages * 100)\n",
    "    \n",
    "    # Calculate accuracy metrics with different tolerance levels\n",
    "    tolerance_levels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    accuracy_at_tolerance = {}\n",
    "    \n",
    "    for tolerance in tolerance_levels:\n",
    "        within_tolerance = np.abs(true_ages - predicted_ages) <= tolerance\n",
    "        accuracy = np.mean(within_tolerance) * 100\n",
    "        accuracy_at_tolerance[f\"accuracy_within_{tolerance}\"] = accuracy\n",
    "    \n",
    "    # Create visualization of the results\n",
    "    create_evaluation_plots(results, true_ages, predicted_ages, accuracy_at_tolerance)\n",
    "    \n",
    "    # Log metrics\n",
    "    logger.info(\"\\n===== Improved Evaluation Metrics =====\")\n",
    "    logger.info(f\"Mean Absolute Error (MAE): {mae:.2f} years\")\n",
    "    logger.info(f\"Root Mean Squared Error (RMSE): {rmse:.2f} years\")\n",
    "    logger.info(f\"RÂ² Score: {r2:.4f}\")\n",
    "    logger.info(f\"Mean Percentage Error: {mpe:.2f}%\")\n",
    "    \n",
    "    for tolerance, accuracy in accuracy_at_tolerance.items():\n",
    "        logger.info(f\"{tolerance.replace('_', ' ').title()}: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mean_absolute_error\": mae,\n",
    "            \"root_mean_squared_error\": rmse,\n",
    "            \"r2_score\": r2,\n",
    "            \"mean_percentage_error\": mpe\n",
    "        },\n",
    "        \"accuracy_at_tolerance\": accuracy_at_tolerance,\n",
    "        \"processed_images\": len(valid_results),\n",
    "        \"total_images\": len(results),\n",
    "        \"success_rate\": len(valid_results) / len(results) if results else 0\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline():\n",
    "    \"\"\"\n",
    "    Evaluate the entire pipeline on the ground truth dataset\n",
    "    First save the basic results, then calculate metrics\n",
    "    \"\"\"\n",
    "    # First, just collect the basic results\n",
    "    basic_results = []\n",
    "    \n",
    "    # Process each image\n",
    "    for img_data in ground_truth_json:\n",
    "        logger.info(f\"Processing {img_data['path']}...\")\n",
    "        predicted_age, visualization = process_image(img_data)\n",
    "        \n",
    "        # Store the basic result\n",
    "        result = {\n",
    "            \"path\": img_data[\"path\"],\n",
    "            \"true_age\": img_data[\"age\"]\n",
    "        }\n",
    "        \n",
    "        if predicted_age is not None:\n",
    "            result[\"predicted_age\"] = predicted_age\n",
    "            \n",
    "            # Save visualization\n",
    "            # output_path = os.path.join(OUTPUT_DIR, f\"eval_{os.path.basename(img_data['path'])}\")\n",
    "            # cv2.imwrite(output_path, visualization)\n",
    "            # logger.info(f\"  Result: True age = {img_data['age']}, Predicted age = {predicted_age}\")\n",
    "        else:\n",
    "            result[\"predicted_age\"] = None\n",
    "            logger.error(f\"  Failed to process {img_data['path']}\")\n",
    "        \n",
    "        basic_results.append(result)\n",
    "    \n",
    "    return basic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "logger.info(\"Starting evaluation of the tree disk analysis pipeline...\")\n",
    "basic_results = evaluate_pipeline()\n",
    "\n",
    "# Save basic results first\n",
    "basic_results_file = \"basic_results.json\"\n",
    "with open(basic_results_file, \"w\") as f:\n",
    "    json.dump(basic_results, f, indent=2)\n",
    "    \n",
    "logger.info(f\"Basic results saved to {basic_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_results_file = \"basic_results.json\"\n",
    "basic_results = json.load(open(basic_results_file))\n",
    "\n",
    "# calculate metrics on top of the basic results\n",
    "metrics = calculate_metrics(basic_results)\n",
    "\n",
    "# Save metrics to a separate file\n",
    "metrics_file = \"metrics.json\"\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "    \n",
    "logger.info(f\"Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = \"/Volumes/Tony SSD/Projekte/Studienarbeit/Datasets/uruDendro/images\"\n",
    "annotations_data = \"/Volumes/Tony SSD/Projekte/Studienarbeit/Datasets/uruDendro/annotations\"\n",
    "\n",
    "# retrieve all .png files in the folder\n",
    "files = [f for f in os.listdir(images_data) if f.endswith('.png')]\n",
    "\n",
    "# delete macosx files\n",
    "files = [f for f in files if not f.startswith('._')]\n",
    "files = [f for f in files if not f.startswith('.')]\n",
    "\n",
    "print(files)\n",
    "\n",
    "# go trough annotations_data and retrieve the json files mathcing the image files, the image name must be exactly the same\n",
    "annotations = []\n",
    "for f in files:\n",
    "    for a in os.listdir(annotations_data):\n",
    "        if f.split('.')[0] in a:\n",
    "            annotations.append(a)\n",
    "\n",
    "annotations = [f for f in annotations if not f.startswith('._')]\n",
    "annotations = [f for f in annotations if not f.startswith('.')]\n",
    "annotations = [f for f in annotations if not '-M' in f]\n",
    "annotations = [f for f in annotations if not '-V' in f]\n",
    "annotations = [f for f in annotations if not '-S' in f]\n",
    "annotations = [f for f in annotations if not '-C' in f]\n",
    "\n",
    "print(annotations)\n",
    "\n",
    "print(len(files))\n",
    "print(len(annotations))\n",
    "\n",
    "# read the json files and save the lenght (number of times) of the shapes key\n",
    "shapes = []\n",
    "for a in annotations:\n",
    "    with open(os.path.join(annotations_data, a)) as f:\n",
    "        data = json.load(f)\n",
    "        shapes.append(len(data['shapes']))\n",
    "\n",
    "print(shapes)\n",
    "\n",
    "# create a json array with the image name and the number of shapes\n",
    "data = []\n",
    "for i in range(len(files)):\n",
    "    data.append({\n",
    "        \"path\": files[i],\n",
    "        \"age\": shapes[i]\n",
    "    })\n",
    "\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
